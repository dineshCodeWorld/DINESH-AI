# Dinesh AI Configuration File
# Complete settings for model training, data collection, and deployment

# ==================== MODEL CONFIGURATION ====================
model:
  # Model Architecture (Optimized for quality)
  vocab_size: 32000            # Increased from 8000 (Gemini's recommendation)
  d_model: 512                 # Increased from 256 for better representations
  num_layers: 6                # Increased from 4 for deeper learning
  num_heads: 8                 # Matches d_model (512/64=8)
  d_ff: 2048                   # 4x d_model for better feature extraction
  max_length: 512              # Increased from 256 for longer context
  dropout: 0.1                 # Dropout rate
  
  # Generation Parameters (for human-like responses)
  temperature: 0.8             # Higher = more creative/human-like (0.7-0.9 recommended)
  top_k: 50                    # Top-k sampling
  top_p: 0.92                  # Nucleus sampling (higher = more diverse)
  max_new_tokens: 150          # Maximum tokens to generate
  repetition_penalty: 1.2      # Prevent repetitive responses

# ==================== TRAINING CONFIGURATION ====================
training:
  # Hyperparameters (Optimized for GPU training)
  batch_size: 16               # Balanced for GPU memory
  learning_rate: 0.0003        # Standard for transformers
  weight_decay: 0.01           # L2 regularization
  epochs: 10                   # More epochs for better learning
  warmup_steps: 500            # Gradual warmup
  
  # Optimization
  optimizer: "adamw"           # Optimizer: adamw, adam, sgd
  scheduler: "cosine"          # LR scheduler: cosine, linear, constant
  gradient_clip: 1.0           # Gradient clipping value
  
  # Device and precision
  device: "cuda"               # Device: cuda, cpu (auto-detects GPU if available)
  mixed_precision: true        # Enable mixed precision training (requires GPU)
  fallback_to_cpu: true        # Fallback to CPU if GPU not available
  
  # Checkpointing
  save_interval: 100           # Save checkpoint every N batches
  eval_interval: 500           # Evaluate every N batches
  keep_best_checkpoints: 3     # Keep best N checkpoints
  early_stopping_patience: 5   # Epochs without improvement before stopping

# ==================== DATA CONFIGURATION ====================
data:
  train_split: 0.8             # Training data split
  val_split: 0.1               # Validation data split
  test_split: 0.1              # Test data split
  
  # Data processing
  max_samples: null            # Maximum samples (null = use all)
  min_length: 50               # Minimum document length (tokens)
  max_length: 2048             # Maximum document length (tokens)
  shuffle: true                # Shuffle data
  
  # BPE Tokenizer (Optimized)
  bpe_vocab_size: 32000        # Increased from 8000 (Gemini's recommendation)
  bpe_min_frequency: 1         # Lowered from 2 to capture more tokens

# ==================== DATA SOURCES ====================
data_sources:
  # API Configuration
  user_agent: "DineshAI/1.0 (Educational Project; Python/requests)"
  request_timeout: 15          # Seconds
  retry_attempts: 3            # Number of retries on failure
  rate_limit_delay: 1.0        # Seconds between requests
  
  # Wikipedia Configuration
  wikipedia:
    enabled: true
    limit: 5000                 # Quality over quantity
    batch_size: 20             # Articles per API request
    rate_limit_delay: 0.5      # Seconds between requests
    categories:
      - Technology
      - Science
      - History
      - Culture
      - Business
      - Health
      - Philosophy
      - Psychology
  
  # ArXiv Configuration
  arxiv:
    enabled: true
    limit: 300                 # Research papers
    max_results_per_request: 100  # API limit per request
    sort_by: "submittedDate"   # Sort by: submittedDate, relevance
    rate_limit_delay: 1.0      # ArXiv requires 1 req/sec
    categories:
      - cs.AI                  # Artificial Intelligence
      - cs.LG                  # Machine Learning
      - cs.NE                  # Neural and Evolutionary Computing
      - stat.ML                # Statistics - Machine Learning
      - cs.CL                  # Computation and Language
  
  # Project Gutenberg Configuration
  gutenberg:
    enabled: true
    limit: 200                 # Increased: novels have dialogues
    mirror: "https://www.gutenberg.org"  # Gutenberg mirror
    api_url: "https://gutendex.com/books"
    max_pages: 10              # Maximum pages to fetch
    rate_limit_delay: 1.0      # Seconds between requests
    # Note: Fiction books contain natural conversations and dialogues
  
  # Reddit Configuration
  reddit:
    enabled: true
    limit: 100                 # Reddit posts and discussions
    posts_per_subreddit: 20    # Posts to fetch per subreddit
    rate_limit_delay: 2.0      # Seconds between requests
    subreddits:
      - AskReddit
      - explainlikeimfive
      - todayilearned
      - science
      - technology
  
  # HackerNews Configuration
  hackernews:
    enabled: true
    limit: 50                  # Top HN stories
    rate_limit_delay: 0.5      # Seconds between requests
  
  # News RSS Feeds
  news:
    enabled: true
    limit: 50                  # News articles from RSS feeds
    rate_limit_delay: 2.0      # Seconds between feeds

# ==================== CONTINUOUS LEARNING ====================
continuous:
  enabled: true                # Enable continuous learning
  
  # Collection Schedule
  collection_interval_hours: 24         # Collect data every N hours
  fine_tune_interval_hours: 72          # Fine-tune every N hours (3 days)
  sample_size: 1000                     # New samples per collection
  
  # Data Management
  retention_days: 30           # Keep data for N days
  auto_cleanup: true           # Automatically delete old data
  
  # Weekly Versioning (Your Requirement!)
  weekly_versioning: true      # Create new version every week
  version_release_day: "sunday"         # Release day (sunday, monday, etc.)
  version_release_time: "00:00"         # Release time (UTC)
  
  # Auto-Deployment
  auto_deploy: true            # Automatically deploy improved models
  deploy_on_improvement: true  # Deploy only if metrics improve
  metric_threshold: 0.99       # Deploy if metric > threshold
  
  # Safety Checks
  max_metric_drop_percent: 5   # Rollback if metrics drop >5%
  max_errors_before_pause: 3   # Pause if 3 errors in a row
  pause_duration_hours: 1      # Pause duration after error
  
  # Backup Strategy
  backup_interval_hours: 24    # Backup every 24 hours
  max_backups: 5               # Keep last 5 backups

# ==================== DEPLOYMENT CONFIGURATION ====================
deployment:
  # Hugging Face Hub
  push_to_hub: true            # Push trained models to Hugging Face
  hf_repo_name: "dinesh-ai-custom-gpt"  # Repository name on HF
  hf_private: false            # Make repository private
  
  # Versioning
  version_format: "v{number}-{date}"    # v1-24-02-2026, v2-02-03-2026
  archive_old_versions: true   # Archive old versions
  
  # Deployment Targets
  targets:
    hugging_face: true         # Deploy to Hugging Face Hub
    local: true                # Keep local copy
    oracle_cloud: false        # Deploy to Oracle Cloud (if configured)

# ==================== LOGGING CONFIGURATION ====================
logging:
  level: "INFO"                # Log level: DEBUG, INFO, WARNING, ERROR
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  
  # File logging
  log_file: "logs/training.log"
  max_file_size: 10485760      # 10MB
  backup_count: 5              # Keep 5 backup logs
  
  # Console logging
  console_output: true         # Print to console
  
  # Specific loggers
  tensorboard: true            # Enable TensorBoard logging
  wandb: false                 # Enable Weights & Biases logging

# ==================== METRICS TRACKING ====================
metrics:
  # Model Quality Metrics
  track_perplexity: true       # Track perplexity (model confusion)
  track_bleu: true             # Track BLEU score (n-gram overlap)
  track_vocab_match: true      # Track % of real English words
  
  # Evaluation Settings
  eval_every_n_steps: 100      # Evaluate metrics every N steps
  save_best_model: true        # Save model when metrics improve
  
  # Visualization
  tensorboard_dir: "runs"      # TensorBoard log directory
  save_sample_outputs: true    # Save sample generations each epoch
  samples_per_eval: 5          # Number of samples to generate
  
  # Test Prompts (for consistency tracking)
  test_prompts:
    - "hi"
    - "What is AI?"
    - "Tell me about science"
    - "How does technology work?"
  
  # Dictionary for vocab matching
  use_english_dict: true       # Use English dictionary for vocab matching
  min_word_length: 2           # Minimum word length to count

# ==================== PATHS CONFIGURATION ====================
paths:
  data_dir: "data"             # Data directory
  raw_data_dir: "data/raw"     # Raw data directory
  processed_data_dir: "data/processed"  # Processed data directory
  
  models_dir: "models"         # Models directory
  models_versions_dir: "models/versions"  # Versioned models directory
  
  logs_dir: "logs"             # Logs directory
  checkpoints_dir: "checkpoints"        # Checkpoints directory
  
  # Specific model files
  tokenizer_file: "tokenizer.json"      # BPE tokenizer file
  model_file: "model.pt"                # Model weights file
  config_file: "model_config.json"      # Model config file

# ==================== SYSTEM CONFIGURATION ====================
system:
  # Parallel Processing
  num_workers: 4               # Number of data loading workers
  pin_memory: true             # Pin memory for data loading
  
  # Random Seeds
  seed: 42                     # Random seed for reproducibility
  deterministic: true          # Use deterministic algorithms
  
  # Performance
  use_compiled_model: false    # Use torch.compile for faster training
  cache_data: true             # Cache processed data in memory

# ==================== APP CONFIGURATION ====================
app:
  # Streamlit UI Settings
  page_title: "Dinesh AI"
  page_icon: "‚ú®"
  layout: "wide"
  
  # Model Cache
  model_cache_ttl: 60          # Seconds to cache model list
  
  # Theme Colors (Dark Mode)
  dark_theme:
    background: "#0e1117"
    text: "#e0e0e0"
    accent: "#667eea"
    user_msg_bg: "#1e3a5f"
    bot_msg_bg: "#1e1e1e"
    input_bg: "#1e1e1e"
    border: "#333"
  
  # Theme Colors (Light Mode)
  light_theme:
    background: "#ffffff"
    text: "#1a1a1a"
    accent: "#667eea"
    user_msg_bg: "#e3f2fd"
    bot_msg_bg: "#f5f5f5"
    input_bg: "#ffffff"
    border: "#ddd"
  
  # Generation Defaults (UI)
  default_temperature: 0.8
  default_top_k: 50
  default_max_length: 150
  
  # Example Prompts
  example_prompts:
    - icon: "ü§ñ"
      title: "What is AI?"
      prompt: "What is artificial intelligence?"
    - icon: "üåç"
      title: "Science"
      prompt: "Tell me about quantum physics"
    - icon: "üíª"
      title: "Technology"
      prompt: "How does blockchain work?"

# ==================== NOTES ====================
# Configuration Priority (highest to lowest):
# 1. Command line arguments
# 2. Environment variables
# 3. This config.yaml file
# 4. Default values in code
#
# To override any setting from command line:
#   python scripts/train.py --device cuda --batch_size 32
#
# To override from environment variables:
#   export TRAINING_BATCH_SIZE=32
#   export MODEL_D_MODEL=1024
#   python scripts/train.py
#
# For weekly versioning with GitHub Actions:
# - All versioning happens automatically
# - New versions created every Sunday at 00:00 UTC
# - Format: v1-24-02-2026, v2-02-03-2026, etc.
# - Auto-deployed to Hugging Face Hub
#
# Last Updated: February 26, 2026
