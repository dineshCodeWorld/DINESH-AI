# ğŸ—ï¸ Architecture Overview

Understanding how Dinesh AI works under the hood.

## ğŸ¯ System Overview

Dinesh AI is a custom GPT-style language model built from scratch with:
- **Custom transformer architecture** (not pretrained)
- **Multi-source data collection** (Wikipedia, ArXiv, Gutenberg)
- **Incremental learning** (fine-tuning on new data)
- **Automatic deduplication** (prevents duplicate training)
- **Model versioning** (weekly snapshots)
- **Web interface** (Streamlit-based chat)

## ğŸ“Š High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Dinesh AI System                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Data Sources â”‚â”€â”€â”€â–¶â”‚  Collection  â”‚â”€â”€â”€â–¶â”‚Processing â”‚ â”‚
â”‚  â”‚              â”‚    â”‚              â”‚    â”‚           â”‚ â”‚
â”‚  â”‚ â€¢ Wikipedia  â”‚    â”‚ â€¢ Dedup      â”‚    â”‚ â€¢ Clean   â”‚ â”‚
â”‚  â”‚ â€¢ ArXiv      â”‚    â”‚ â€¢ Schedule   â”‚    â”‚ â€¢ Filter  â”‚ â”‚
â”‚  â”‚ â€¢ Gutenberg  â”‚    â”‚ â€¢ Parallel   â”‚    â”‚ â€¢ Format  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                                       â”‚        â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                             â–¼                            â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚                    â”‚ Tokenization â”‚                      â”‚
â”‚                    â”‚              â”‚                      â”‚
â”‚                    â”‚ â€¢ BPE        â”‚                      â”‚
â”‚                    â”‚ â€¢ 5K-50K     â”‚                      â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                             â”‚                            â”‚
â”‚                             â–¼                            â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚                    â”‚   Training   â”‚                      â”‚
â”‚                    â”‚              â”‚                      â”‚
â”‚                    â”‚ â€¢ From       â”‚                      â”‚
â”‚                    â”‚   Scratch    â”‚                      â”‚
â”‚                    â”‚ â€¢ Fine-tune  â”‚                      â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                             â”‚                            â”‚
â”‚                             â–¼                            â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚         â”‚         Model Storage            â”‚            â”‚
â”‚         â”‚                                  â”‚            â”‚
â”‚         â”‚ â€¢ Daily models                   â”‚            â”‚
â”‚         â”‚ â€¢ Weekly versions                â”‚            â”‚
â”‚         â”‚ â€¢ Automatic backups              â”‚            â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                             â”‚                            â”‚
â”‚                             â–¼                            â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚                    â”‚ Web Interfaceâ”‚                      â”‚
â”‚                    â”‚              â”‚                      â”‚
â”‚                    â”‚ â€¢ Chat       â”‚                      â”‚
â”‚                    â”‚ â€¢ Testing    â”‚                      â”‚
â”‚                    â”‚ â€¢ Info       â”‚                      â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ§  Model Architecture

### Custom GPT Transformer

```
Input Text
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tokenization    â”‚  BPE tokenizer
â”‚ (5K-50K vocab)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Token Embedding â”‚  vocab_size Ã— d_model
â”‚ + Positional    â”‚  max_length Ã— d_model
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Transformer     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Decoder Block 1 â”‚â”€â”€â”‚ Self-Attention   â”‚
â”‚                 â”‚  â”‚ (Multi-head)     â”‚
â”‚                 â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                 â”‚  â”‚ Feed-Forward     â”‚
â”‚                 â”‚  â”‚ Network          â”‚
â”‚                 â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Transformer     â”‚  (Repeat num_layers times)
â”‚ Decoder Block N â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer Norm      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Output          â”‚  d_model â†’ vocab_size
â”‚ Projection      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
Generated Text
```

### Transformer Decoder Block

```
Input (batch, seq_len, d_model)
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer Normalization             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Multi-Head Self-Attention       â”‚
â”‚                                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”        â”‚
â”‚ â”‚  Q  â”‚ â”‚  K  â”‚ â”‚  V  â”‚        â”‚
â”‚ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜        â”‚
â”‚    â”‚       â”‚       â”‚            â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚           â”‚                     â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚    â”‚  Attention  â”‚              â”‚
â”‚    â”‚  (Causal)   â”‚              â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Residual Connection + Dropout   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer Normalization             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feed-Forward Network            â”‚
â”‚                                 â”‚
â”‚ Linear(d_model â†’ d_ff)          â”‚
â”‚         â†“                       â”‚
â”‚       GELU                      â”‚
â”‚         â†“                       â”‚
â”‚ Linear(d_ff â†’ d_model)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Residual Connection + Dropout   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
Output (batch, seq_len, d_model)
```

## ğŸ”„ Training Pipeline

### Complete Workflow

```
1. Data Collection
   â”œâ”€ Wikipedia API â†’ Articles
   â”œâ”€ ArXiv API â†’ Papers
   â””â”€ Gutenberg â†’ Books
   
2. Deduplication
   â”œâ”€ MD5 hash each item
   â”œâ”€ Check seen_content_hashes.json
   â””â”€ Skip duplicates
   
3. Preprocessing
   â”œâ”€ Clean text
   â”œâ”€ Remove HTML/special chars
   â”œâ”€ Filter by length
   â””â”€ Merge with existing data
   
4. Tokenization
   â”œâ”€ Train BPE tokenizer
   â”œâ”€ Build vocabulary (5K-50K)
   â””â”€ Save tokenizer.json
   
5. Model Training
   â”œâ”€ Load previous model (if exists)
   â”œâ”€ Fine-tune on new data
   â”œâ”€ Save checkpoints
   â””â”€ Save final model
   
6. Deployment
   â”œâ”€ Create model card
   â”œâ”€ Save configuration
   â”œâ”€ Create backups
   â””â”€ Ready for use
```

### Training Modes

**From Scratch (Day 1):**
```python
model = CustomGPT(config)
model.train(data)
model.save()
```

**Fine-Tuning (Day 2+):**
```python
model = load_previous_model()
model.fine_tune(new_data, lr=1e-5, epochs=1)
model.save()
```

## ğŸ’¾ Data Flow

### Collection â†’ Training

```
Wikipedia/ArXiv/Gutenberg
         â”‚
         â–¼
    [Raw Data]
    data/raw/
         â”‚
         â–¼
  [Deduplication]
  MD5 hashing
         â”‚
         â–¼
   [Processing]
   Clean & filter
         â”‚
         â–¼
  [Processed Data]
  data/processed/
         â”‚
         â–¼
  [Tokenization]
  BPE encoding
         â”‚
         â–¼
   [Training]
   Model learning
         â”‚
         â–¼
  [Trained Model]
  models/model_*/
```

### Continuous Learning

```
Day 1: Collect â†’ Train from scratch â†’ Save v1
Day 2: Collect â†’ Fine-tune v1 â†’ Save v2
Day 3: Collect â†’ Fine-tune v2 â†’ Save v3
...
Week 1: Create version snapshot â†’ v1-YYYY-MM-DD
```

## ğŸ”§ Key Components

### 1. Data Collector (`src/data/data_collector.py`)
- Fetches from multiple sources
- MD5-based deduplication
- Random sampling for freshness
- Parallel collection

### 2. Data Preprocessor (`src/data/data_preprocessor.py`)
- Text cleaning
- Length filtering
- Format standardization
- Merging with existing data

### 3. Custom Model (`src/core/custom_model.py`)
- Transformer architecture
- Multi-head attention
- Causal masking
- Text generation

### 4. Model Trainer (`src/core/model_trainer.py`)
- Training from scratch
- Fine-tuning
- Checkpoint management
- Loss tracking

### 5. Continuous Trainer (`src/continuous/continuous_trainer.py`)
- Scheduled collection (every 6 hours)
- Scheduled training (every 24 hours)
- Model search & backup
- Parallel execution

### 6. Web Interface (`app.py`)
- Streamlit-based UI
- Model loading
- Text generation
- Statistics display

## ğŸ“Š Model Specifications

### Local Configuration
```
Parameters: ~2M
Layers: 2
Dimension: 128
Heads: 2
FFN: 512
Vocabulary: 5,000
Max Length: 128
Size: ~5MB
```

### Production Configuration
```
Parameters: ~100M
Layers: 12
Dimension: 768
Heads: 12
FFN: 3,072
Vocabulary: 50,000
Max Length: 512
Size: ~380MB
```

## ğŸ¯ Design Decisions

### Why Custom Architecture?
- **Full control** over model design
- **Learning experience** - understand transformers deeply
- **Customization** - optimize for specific needs
- **No black boxes** - complete transparency

### Why From Scratch?
- **Educational value** - learn by building
- **Flexibility** - modify as needed
- **Understanding** - know every component
- **Innovation** - implement new ideas

### Why Fine-Tuning?
- **Incremental learning** - don't forget old knowledge
- **Efficiency** - faster than retraining
- **Continuous improvement** - always learning
- **Resource-friendly** - less computation

### Why Deduplication?
- **Efficiency** - don't process same data twice
- **Quality** - avoid overfitting on duplicates
- **Storage** - save disk space
- **Speed** - faster training

## ğŸ” Technical Details

### Attention Mechanism
```python
scores = Q @ K.T / sqrt(d_k)
scores = mask_future(scores)  # Causal masking
attention = softmax(scores)
output = attention @ V
```

### Causal Masking
```
Prevents attending to future tokens:
[1, 0, 0, 0]
[1, 1, 0, 0]
[1, 1, 1, 0]
[1, 1, 1, 1]
```

### BPE Tokenization
```
"artificial intelligence" â†’
["art", "ificial", " intel", "ligence"]
```

### Parameter Count
```
Embeddings: vocab_size Ã— d_model
Positional: max_length Ã— d_model
Per Layer: 4 Ã— d_modelÂ² (attention + FFN)
Output: d_model Ã— vocab_size
Total: ~100M for production config
```

## ğŸ“ˆ Performance Characteristics

### Training Speed
- CPU: ~40 min/epoch (production)
- GPU: ~5-10 min/epoch (production)
- Local: ~5 min/epoch

### Inference Speed
- CPU: ~1-2 sec/response
- GPU: ~0.1-0.5 sec/response

### Memory Usage
- Training: ~4GB (production)
- Inference: ~500MB (production)
- Local: ~500MB total

---

**Next:** [Training Pipeline](05_TRAINING_PIPELINE.md)

*Last Updated: February 26, 2026*
